{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01b94da5-30ea-4340-b35e-af3f621a6644",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Challenger model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bca9890b-2ec6-4b0a-bbe8-df3125158f58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 01. Fetch Model information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5062b8c4-2117-47dd-98af-35e6a3fb2503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We are interested in validating the Challenger model\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "catalog = \"workspace\"\n",
    "db = \"booking\"\n",
    "model_alias = \"Challenger\"\n",
    "model_name = f\"{catalog}.{db}.mlops_booking\"\n",
    "\n",
    "client = MlflowClient()\n",
    "model_details = client.get_model_version_by_alias(model_name, model_alias)\n",
    "model_version = int(model_details.version)\n",
    "\n",
    "print(f\"Validating {model_alias} model for {model_name} on model version {model_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3ecb341-a9b4-45c3-a7f8-06645d43edb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 02. Model checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9346341-f6de-4a1a-9025-519cd256c745",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Description check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88251f7d-9a3d-4a64-9ee2-5ff1cb1dd30e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If there's no description or an insufficient number of characters, tag accordingly\n",
    "if not model_details.description:\n",
    "  has_description = False\n",
    "  print(\"Please add model description\")\n",
    "elif not len(model_details.description) > 20:\n",
    "  has_description = False\n",
    "  print(\"Please add detailed model description (40 char min).\")\n",
    "else:\n",
    "  has_description = True\n",
    "\n",
    "print(f'Model {model_name} version {model_details.version} has description: {has_description}')\n",
    "client.set_model_version_tag(name=model_name, version=str(model_details.version), key=\"has_description\", value=has_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca688cc0-338a-41ce-97ae-52f6f37c96ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Model performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "003c8c37-9c60-43ff-ae40-2954cf513074",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "model_run_id = model_details.run_id\n",
    "f1_score = mlflow.get_run(model_run_id).data.metrics['test_f1_score']\n",
    "\n",
    "try:\n",
    "    #Compare the challenger f1 score to the existing champion if it exists\n",
    "    champion_model = client.get_model_version_by_alias(model_name, \"Champion\")\n",
    "    champion_f1 = mlflow.get_run(champion_model.run_id).data.metrics['test_f1_score']\n",
    "    print(f'Champion f1 score: {champion_f1}. Challenger f1 score: {f1_score}.')\n",
    "    metric_f1_passed = f1_score >= champion_f1\n",
    "except:\n",
    "    print(f\"No Champion found. Accept the model as it's the first one.\")\n",
    "    metric_f1_passed = True\n",
    "\n",
    "print(f'Model {model_name} version {model_details.version} metric_f1_passed: {metric_f1_passed}')\n",
    "# Tag that F1 metric check has passed\n",
    "client.set_model_version_tag(name=model_name, version=model_details.version, key=\"metric_f1_passed\", value=metric_f1_passed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d63c214-3b0a-4c1a-9cc0-8d6a3103aa63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Benchmark or business metrics on the eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19a4ab87-2b0d-4302-8a44-a6822014f500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "#get our validation dataset:\n",
    "validation_df = spark.table(f\"{catalog}.{db}.mlops_booking_training\").filter(\"split='validate'\")\n",
    "\n",
    "#Call the model with the given alias and return the prediction\n",
    "def predict_churn(validation_df, model_alias):\n",
    "    model = mlflow.pyfunc.spark_udf(spark, model_uri=f\"models:/{catalog}.{db}.mlops_booking@{model_alias}\") #Use env_manager=\"virtualenv\" to recreate a venv with the same python version if needed\n",
    "    return validation_df.withColumn('predictions', model(*model.metadata.get_input_schema().input_names()))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "challenger model validation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
