{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40bdc4d7-f157-42dd-bb57-c23bba521318",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Booking Cancellation Prediction Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20d26c52-cff1-41f7-943e-75b1bcd940a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 01. Generate Sythetic Data for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eebe685-4a74-4c1f-baec-b16325e525b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# please help generate sythetic data for testing inference based on the following table\n",
    "# workspace.booking.mlops_booking_training\n",
    "\n",
    "from pyspark.sql.functions import col, rand\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def generate_synthetic_data(df: DataFrame, num_rows: int) -> DataFrame:\n",
    "    unique_values = df.distinct()\n",
    "    synthetic_data = unique_values.orderBy(rand()).limit(num_rows)\n",
    "    return synthetic_data.withColumn(\"Booking_ID\", lit(None))\n",
    "\n",
    "# Load the original data\n",
    "original_data = spark.table(\"workspace.booking.mlops_booking_training\")\n",
    "\n",
    "# Generate synthetic data\n",
    "synthetic_data = generate_synthetic_data(original_data, 100)\n",
    "\n",
    "# Display the synthetic data\n",
    "display(synthetic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6782b6b2-49a1-47a5-a123-2e8c96d8b2e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the synthetic data to a table\n",
    "synthetic_data.write.mode(\"overwrite\").saveAsTable(\"workspace.booking.mlops_booking_inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83e1effd-09e5-44d6-8c07-52fa1e3da7df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 02. Batch inference on the Champion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feba7e01-c420-48f5-82e4-79230c3c00b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Catalog and Schema\n",
    "catalog = \"workspace\"\n",
    "db = \"booking\"\n",
    "\n",
    "# Load customer features to be scored\n",
    "inference_df = spark.read.table(f\"{catalog}.{db}.mlops_booking_inference\")\n",
    "\n",
    "# Load champion model as a Spark UDF\n",
    "champion_model = mlflow.pyfunc.spark_udf(\n",
    "    spark, \n",
    "    model_uri=f\"models:/{catalog}.{db}.mlops_booking@Challenger\"\n",
    ")\n",
    "\n",
    "# Ensure the input columns match the model's input schema\n",
    "input_columns = champion_model.metadata.get_input_schema().input_names()\n",
    "preds_df = inference_df.select(*input_columns).withColumn(\n",
    "    'predictions', \n",
    "    champion_model(*input_columns)\n",
    ")\n",
    "\n",
    "display(preds_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "batch_inference",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
