{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b85331e5-ce75-435b-999e-a168e7689002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a146c1fc-c45b-44de-abbd-b9ade9dc8d78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 01 - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2278bcf4-f3f4-4014-a70b-e0040158f331",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "booking_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(f\"file:{os.path.dirname(os.getcwd())}/data/booking.csv\")\n",
    "display(booking_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28eda214-82af-4007-a869-409de04763c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Replace spaces in column names with underscores\n",
    "for col in booking_df.columns:\n",
    "    booking_df = booking_df.withColumnRenamed(col, col.replace(' ', '_'))\n",
    "\n",
    "display(booking_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b0a1346-51f3-4bd6-bbd1-d067cf399310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prinf the schema\n",
    "booking_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "517469dc-fcad-421b-863d-cc71f1dc9194",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 02 - Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dd93b83-20cd-40da-b5ee-d1efe19b029c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "# Convert Spark DataFrame to pandas-on-Spark DataFrame\n",
    "psdf = booking_df.pandas_api()\n",
    "\n",
    "# Strip whitespace and convert to datetime, coerce errors to NaT\n",
    "psdf['date_of_reservation'] = psdf['date_of_reservation'].str.strip()\n",
    "psdf['date_of_reservation'] = ps.to_datetime(psdf['date_of_reservation'], errors='coerce', infer_datetime_format=True)\n",
    "\n",
    "# Fill NaT with default date\n",
    "psdf['date_of_reservation'] = psdf['date_of_reservation'].fillna(\"1970-01-01\")\n",
    "\n",
    "# Convert back to Spark DataFrame\n",
    "booking_df = psdf.to_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "073ab448-368b-4882-a53d-0c4a8c87670e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, col\n",
    "\n",
    "# Extract day, month, and year from date_of_reservation\n",
    "booking_df = booking_df.withColumn(\"reservation_year\", year(col(\"date_of_reservation\"))) \\\n",
    "                       .withColumn(\"reservation_month\", month(col(\"date_of_reservation\"))) \\\n",
    "                       .withColumn(\"reservation_day\", dayofmonth(col(\"date_of_reservation\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ac157bf-ea6e-418c-a52c-7a44291f67f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate features\n",
    "booking_df = booking_df.withColumn(\"total_nights\", col(\"number_of_weekend_nights\") + col(\"number_of_week_nights\"))\n",
    "\n",
    "# Interaction features\n",
    "booking_df = booking_df.withColumn(\"lead_time_adults\", col(\"lead_time\") * col(\"number_of_adults\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3afad8c3-2ed8-428c-8f34-3ad1fd0dd9c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in [\"type_of_meal\", \"room_type\", \"market_segment_type\"]]\n",
    "encoders = [OneHotEncoder(inputCol=column+\"_index\", outputCol=column+\"_encoded\") for column in [\"type_of_meal\", \"room_type\", \"market_segment_type\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4805c9c5-f3dc-4cda-bae7-f93143d68dff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Binarize target variable if necessary\n",
    "booking_df = booking_df.withColumn(\"booking_status_binary\", when(col(\"booking_status\") == \"Canceled\", 0).otherwise(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec065de1-fcee-4006-b983-40bc513f2477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove the existing column if it exists\n",
    "for column in [\"type_of_meal_encoded\", \"room_type_encoded\", \"market_segment_type_encoded\"]:\n",
    "    if column in booking_df.columns:\n",
    "        booking_df = booking_df.drop(column)\n",
    "\n",
    "# Create a pipeline for the transformations\n",
    "pipeline = Pipeline(stages=indexers + encoders)\n",
    "booking_df = pipeline.fit(booking_df).transform(booking_df)\n",
    "\n",
    "display(booking_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2564e48-bc5b-48e2-a97f-fdb29cb63331",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop intermediate columns if necessary\n",
    "booking_df = booking_df.drop(\"type_of_meal_index\", \"room_type_index\", \"market_segment_type_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4fb7ae8-ab5c-4ae7-8f2e-4fbd0bea44e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(booking_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "375fee89-e112-4319-adf2-92e6ee440576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 03. Write Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5128aeee-e577-4e51-a4c3-83fc28f7c751",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a new Schema under the \"booking\" schema\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS booking\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97e59b64-5497-4fda-bf02-79a00e232a4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the booking schema\n",
    "spark.sql(\"USE booking\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e984dde6-4c3d-4305-a732-a16cbb843d01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Specify train-val-test split\n",
    "train_ratio, val_ratio, test_ratio = 0.7, 0.2, 0.1\n",
    "booking_df = (booking_df.withColumn(\"random\", F.rand(seed=42))\n",
    "                                .withColumn(\"split\",\n",
    "                                            F.when(F.col(\"random\") < train_ratio, \"train\")\n",
    "                                            .when(F.col(\"random\") < train_ratio + val_ratio, \"validate\")\n",
    "                                            .otherwise(\"test\"))\n",
    "                                .drop(\"random\"))\n",
    "\n",
    "# Write table for training\n",
    "(booking_df.write.mode(\"overwrite\")\n",
    "               .option(\"overwriteSchema\", \"true\")\n",
    "               .saveAsTable(\"mlops_booking_training\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "feature_engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
